{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f037f32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 536829\n",
      "Number of users with at least 5 high ratings: 418829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "DATA = Path(\"./data\")\n",
    "\n",
    "GEN_DATA = DATA / \"gen\"\n",
    "RAW_DATA = DATA / \"raw\"\n",
    "game_data = pl.read_csv(RAW_DATA / \"games_detailed_info2025.csv\")\n",
    "\n",
    "game_data = game_data.with_columns(pl.col(\"description\").fill_null(\"\"))\n",
    "# game_data = game_data.sample(fraction=1)\n",
    "\n",
    "game_data.head()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])  # faster\n",
    "\n",
    "\n",
    "data_path = Path(\"data\")\n",
    "review_data = pl.read_csv(data_path / \"bgg-26m-reviews.csv\")\n",
    "\n",
    "review_data = review_data.filter(pl.col(\"rating\") >= 8)\n",
    "user_data = review_data.group_by(\"user\").agg(\n",
    "    pl.len().alias(\"len\"),\n",
    "    pl.col(\"ID\"),\n",
    ")\n",
    "# keep only users with at least 5 reviews\n",
    "print(f\"Number of users: {user_data.height}\")\n",
    "user_data = user_data.filter(pl.col(\"len\") > 1)\n",
    "print(f\"Number of users with at least 5 high ratings: {user_data.height}\")\n",
    "\n",
    "\n",
    "def spacy_preprocessing(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if token.is_alpha and not token.is_stop])\n",
    "\n",
    "\n",
    "# vectorizer = TfidfVectorizer(analyzer=\"word\", lowercase=True, strip_accents=\"unicode\", max_features=1024, min_df=1, max_df=0.8, token_pattern=r\"[A-Za-z]+\", stop_words=\"english\")\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", lowercase=True, strip_accents=\"unicode\", max_features=1024, min_df=1, max_df=0.8, token_pattern=r\"[A-Za-z]+\", preprocessor=spacy_preprocessing, stop_words=None)\n",
    "\n",
    "X = vectorizer.fit_transform(game_data[\"description\"])\n",
    "\n",
    "reduced_X = X.toarray()\n",
    "\n",
    "print()\n",
    "\n",
    "index_to_id = {index: id for index, id in enumerate(game_data[\"id\"])}\n",
    "id_to_index = {v: k for k, v in index_to_id.items()}\n",
    "\n",
    "# Nearest Neighbors model\n",
    "nbrs = NearestNeighbors(n_neighbors=11, metric=\"cosine\").fit(reduced_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ae1c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 418829 user baskets...\n",
      "Generated embeddings for 418809 users (dropped 20 users with no valid items).\n"
     ]
    }
   ],
   "source": [
    "def compute_basket_embeddings_train_only(baskets, item_vectors, id_map):\n",
    "    user_vectors = []\n",
    "    valid_user_indices = []\n",
    "\n",
    "    valid_ids_set = set(id_map.keys())\n",
    "    print(f\"Processing {len(baskets)} user baskets...\")\n",
    "\n",
    "    for i, basket in enumerate(baskets):\n",
    "        length = len(basket)\n",
    "        if length == 0:\n",
    "            continue\n",
    "\n",
    "        # Split: 30% test, 70% train\n",
    "        split_point = int(np.ceil(0.3 * length))\n",
    "        train_items = basket[split_point:]  # remaining 70% for centroid\n",
    "\n",
    "        # Only keep train items that exist in id_map\n",
    "        train_valid_indices = [id_map[item] for item in train_items if item in valid_ids_set]\n",
    "\n",
    "        if train_valid_indices:\n",
    "            # Compute centroid from train items only\n",
    "            train_vecs = item_vectors[train_valid_indices]\n",
    "            user_vec = np.mean(train_vecs, axis=0)\n",
    "\n",
    "            user_vectors.append(user_vec)\n",
    "            valid_user_indices.append(i)\n",
    "\n",
    "    return np.array(user_vectors), valid_user_indices\n",
    "\n",
    "\n",
    "user_baskets = user_data[\"ID\"].to_list()\n",
    "\n",
    "Y, valid_user_idxs = compute_basket_embeddings_train_only(user_baskets, reduced_X, id_to_index)\n",
    "\n",
    "print(f\"Generated embeddings for {len(Y)} users (dropped {len(user_baskets) - len(Y)} users with no valid items).\")\n",
    "\n",
    "distances, indices = nbrs.kneighbors(Y, n_neighbors=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12247a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recommended_items, true_items, k):\n",
    "    \"\"\" \n",
    "    Calculates precision at k for the recommended items.\n",
    "    Parameters:\n",
    "        recommended_items (list): List of recommended items.\n",
    "        true_items (list): List of true items.\n",
    "        k (int): The cutoff rank (number of recommended items to consider).\n",
    "    \"\"\"\n",
    "    recommended_at_k = recommended_items if len(recommended_items) < k else recommended_items[:k]\n",
    "    true_positives = len(set(recommended_at_k) & set(true_items))\n",
    "    precision = true_positives / (len(recommended_at_k) if len(recommended_at_k) > 0 else 1)\n",
    "    return precision\n",
    "\n",
    "def recall_at_k(recommended_items, true_items, k):\n",
    "    \"\"\" \n",
    "    Calculates recall at k for the recommended items.\n",
    "    Parameters:\n",
    "        recommended_items (list): List of recommended items.\n",
    "        true_items (list): List of true items.\n",
    "        k (int): The cutoff rank (number of recommended items to consider).\n",
    "    \"\"\"\n",
    "    recommended_at_k = recommended_items if len(recommended_items) < k else recommended_items[:k]\n",
    "    true_positives = len(set(recommended_at_k) & set(true_items))\n",
    "    recall = true_positives / (len(true_items) if len(true_items) > 0 else 1)\n",
    "    return recall\n",
    "\n",
    "def fscore_at_k(recommended_items, true_items, k):\n",
    "    \"\"\"\n",
    "    Calculates F1-score at k for the recommended items.\n",
    "    \"\"\"\n",
    "    p = precision_at_k(recommended_items, true_items, k)\n",
    "    r = recall_at_k(recommended_items, true_items, k)\n",
    "\n",
    "    if p + r == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 2 * p * r / (p + r)\n",
    "\n",
    "def evaluate_model(recommended_items_all, basket_test, k):\n",
    "    precisions, recalls, fscores = [], [], []\n",
    "\n",
    "    for row in basket_test.iter_rows():\n",
    "        user_id = row[0]\n",
    "        length = row[1]\n",
    "\n",
    "        true_items = row[2][:int(np.ceil(0.3 * length))]\n",
    "        recommended_items = recommended_items_all.get(user_id, [])\n",
    "\n",
    "        p = precision_at_k(recommended_items, true_items, k)\n",
    "        r = recall_at_k(recommended_items, true_items, k)\n",
    "        f = fscore_at_k(recommended_items, true_items, k)\n",
    "\n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "        fscores.append(f)\n",
    "\n",
    "    return np.mean(precisions), np.mean(recalls), np.mean(fscores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d254f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_prediction_using_embeddings(baskets_df, valid_user_idxs, nn_indices, nn_distances, index_to_id, k=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        baskets_df: The polars dataframe containing User and Games list.\n",
    "        valid_user_idxs: The list returned by compute_basket_embeddings_robust.\n",
    "                         Maps the index in Y/nn_indices back to the row index in baskets_df.\n",
    "        nn_indices: The indices matrix from nbrs.kneighbors.\n",
    "        nn_distances: The distances matrix from nbrs.kneighbors.\n",
    "        index_to_id: Dict mapping matrix index -> Game ID.\n",
    "        k: Number of recommendations to return.\n",
    "    \"\"\"\n",
    "    recommended_items_all = {}\n",
    "    skip_counter = 0\n",
    "    too_short_counter = 0\n",
    "\n",
    "    row_to_array_idx = {row_idx: i for i, row_idx in enumerate(valid_user_idxs)}\n",
    "\n",
    "    for row_idx, row in enumerate(baskets_df.iter_rows()):\n",
    "        user_id = row[0]\n",
    "        all_games = row[2]\n",
    "\n",
    "        if row_idx not in row_to_array_idx:\n",
    "            skip_counter += 1\n",
    "            continue\n",
    "\n",
    "        length = len(all_games)\n",
    "        future = int(np.ceil(0.3 * length))\n",
    "        history = set(all_games[future:])\n",
    "\n",
    "        array_pos = row_to_array_idx[row_idx]\n",
    "\n",
    "        neighbor_matrix_indices = nn_indices[array_pos]\n",
    "\n",
    "\n",
    "        # recommendations are already sorted by distance, select top k not in history NOTE: it can happen that run out of candidates and end up with less than k recommendations\n",
    "        top_k = []\n",
    "        for mat_idx in neighbor_matrix_indices:\n",
    "\n",
    "            if mat_idx not in index_to_id:\n",
    "                continue\n",
    "\n",
    "            game_id = index_to_id[mat_idx]\n",
    "\n",
    "            if game_id not in history:\n",
    "                top_k.append(game_id)\n",
    "\n",
    "            if len(top_k) >= k:\n",
    "                break\n",
    "        if len(top_k) != k:\n",
    "            too_short_counter += 1\n",
    "        recommended_items_all[user_id] = top_k\n",
    "    if skip_counter > 0:\n",
    "        print(f\"Skipped {skip_counter} users (no valid embedding found).\")\n",
    "    if too_short_counter > 0:\n",
    "        print(f\"Could not generate {k} recommendations for {too_short_counter} users (not enough candidates).\")\n",
    "    return recommended_items_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed6631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 418809 users...\n",
      "Skipped 20 users (no valid embedding found).\n",
      "Could not generate 1 recommendations for 48 users (not enough candidates).\n",
      "Skipped 20 users (no valid embedding found).\n",
      "Could not generate 3 recommendations for 3561 users (not enough candidates).\n",
      "Skipped 20 users (no valid embedding found).\n",
      "Could not generate 5 recommendations for 39408 users (not enough candidates).\n",
      "Skipped 20 users (no valid embedding found).\n",
      "Could not generate 7 recommendations for 110692 users (not enough candidates).\n",
      "Skipped 20 users (no valid embedding found).\n",
      "Could not generate 10 recommendations for 316709 users (not enough candidates).\n",
      "    k  precision    recall    fscore\n",
      "0   1   0.013726  0.004534  0.005740\n",
      "1   3   0.010729  0.008357  0.007248\n",
      "2   5   0.009271  0.010565  0.007448\n",
      "3   7   0.008538  0.011954  0.007435\n",
      "4  10   0.008183  0.013080  0.007502\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Generating predictions for {len(valid_user_idxs)} users...\")\n",
    "\n",
    "ks = np.linspace(1, 10, 5, dtype=int)\n",
    "# ks = [3, 5, 7, 10]\n",
    "CV_results = {\"k\": [], \"precision\": [], \"recall\": [], \"fscore\": []}\n",
    "\n",
    "for k in ks:\n",
    "    recommended_items_all = make_prediction_using_embeddings(baskets_df=user_data, valid_user_idxs=valid_user_idxs, nn_indices=indices, nn_distances=distances, index_to_id=index_to_id, k=k)\n",
    "\n",
    "    precision, recall, fscores = evaluate_model(recommended_items_all, user_data, k)\n",
    "    CV_results[\"k\"].append(k)\n",
    "    CV_results[\"precision\"].append(precision)\n",
    "    CV_results[\"recall\"].append(recall)\n",
    "    CV_results[\"fscore\"].append(fscores)\n",
    "\n",
    "CV_results_df = pd.DataFrame(CV_results)\n",
    "print(CV_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
