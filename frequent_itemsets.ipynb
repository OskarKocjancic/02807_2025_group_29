{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d01041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import mlxtend.frequent_patterns as fp\n",
    "import mlxtend.preprocessing as pp\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from efficient_apriori import apriori as apriori_efficient\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "from sklearn.model_selection import KFold\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8b21ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 6)\n",
      "┌─────┬─────────────┬────────┬─────────────────────────────────┬─────┬───────┐\n",
      "│     ┆ user        ┆ rating ┆ comment                         ┆ ID  ┆ name  │\n",
      "│ --- ┆ ---         ┆ ---    ┆ ---                             ┆ --- ┆ ---   │\n",
      "│ i64 ┆ str         ┆ f64    ┆ str                             ┆ i64 ┆ str   │\n",
      "╞═════╪═════════════╪════════╪═════════════════════════════════╪═════╪═══════╡\n",
      "│ 0   ┆ sidehacker  ┆ 10.0   ┆ null                            ┆ 13  ┆ CATAN │\n",
      "│ 1   ┆ Varthlokkur ┆ 10.0   ┆ null                            ┆ 13  ┆ CATAN │\n",
      "│ 2   ┆ dougthonus  ┆ 10.0   ┆ Currently, this sits on my lis… ┆ 13  ┆ CATAN │\n",
      "│ 3   ┆ cypar7      ┆ 10.0   ┆ I know it says how many plays,… ┆ 13  ┆ CATAN │\n",
      "│ 4   ┆ ssmooth     ┆ 10.0   ┆ null                            ┆ 13  ┆ CATAN │\n",
      "└─────┴─────────────┴────────┴─────────────────────────────────┴─────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "raw_data_path = Path('data')\n",
    "generated_data_path = Path('data')\n",
    "\n",
    "data = pl.read_csv(raw_data_path / 'bgg-26m-reviews.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b7fd0",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce1579fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 6)\n",
      "┌─────┬─────────────┬────────┬─────────────────────────────────┬─────┬───────┐\n",
      "│     ┆ user        ┆ rating ┆ comment                         ┆ ID  ┆ name  │\n",
      "│ --- ┆ ---         ┆ ---    ┆ ---                             ┆ --- ┆ ---   │\n",
      "│ i64 ┆ str         ┆ f64    ┆ str                             ┆ i64 ┆ str   │\n",
      "╞═════╪═════════════╪════════╪═════════════════════════════════╪═════╪═══════╡\n",
      "│ 0   ┆ sidehacker  ┆ 10.0   ┆ null                            ┆ 13  ┆ CATAN │\n",
      "│ 1   ┆ Varthlokkur ┆ 10.0   ┆ null                            ┆ 13  ┆ CATAN │\n",
      "│ 2   ┆ dougthonus  ┆ 10.0   ┆ Currently, this sits on my lis… ┆ 13  ┆ CATAN │\n",
      "│ 3   ┆ cypar7      ┆ 10.0   ┆ I know it says how many plays,… ┆ 13  ┆ CATAN │\n",
      "│ 4   ┆ ssmooth     ┆ 10.0   ┆ null                            ┆ 13  ┆ CATAN │\n",
      "└─────┴─────────────┴────────┴─────────────────────────────────┴─────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "data = data.filter(pl.col('rating') >= 8)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3cc005",
   "metadata": {},
   "source": [
    "## Grouping into baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "773a5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets = (\n",
    "    data\n",
    "    .group_by(['user'])\n",
    "    .agg(\n",
    "        pl.col('name').alias('games')\n",
    "    )\n",
    ")\n",
    "\n",
    "baskets_df = baskets.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cad59c",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca131aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_and_train_set(baskets_test_before, baskets_train_before):\n",
    "    \"\"\"\n",
    "    Transforms the test set by splitting the itemsets into test and training parts.\n",
    "    Adds the training parts to the training set.\n",
    "    Parameters:\n",
    "        baskets_test_before (pd.DataFrame): DataFrame with columns 'user' and 'games' for the test set.\n",
    "        baskets_train_before (pd.DataFrame): DataFrame with columns 'user' and 'games' for the training set.\n",
    "    Returns:\n",
    "        baskets_test (pd.DataFrame): Transformed test set with columns 'user', 'games', and 'test'.\n",
    "        baskets_train (pd.DataFrame): Updated training set with columns 'user' and 'games'.\n",
    "    \"\"\"\n",
    "    \n",
    "    baskets_test = baskets_test_before[baskets_test_before['games'].apply(len) >= 4]\n",
    "\n",
    "    baskets_test['test'] = baskets_test['games'].apply(lambda x: x[0:int(np.ceil(0.3*len(x)))])\n",
    "    baskets_test['games'] = baskets_test['games'].apply(lambda x: x[int(np.ceil(0.3*len(x))):])\n",
    "\n",
    "    baskets_test_training = baskets_test[['user', 'games']]\n",
    "    baskets_train = pd.concat([baskets_train_before, baskets_test_training], ignore_index=True)\n",
    "\n",
    "    return baskets_test, baskets_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "585221e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baskets_df_to_tuples(baskets_df):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame with columns 'user' and 'games' to a list of tuples.\n",
    "    Parameters:\n",
    "        baskets_df (pd.DataFrame): DataFrame with columns 'user' and 'games'.\n",
    "    Returns:\n",
    "        baskets_tuples (list of tuples): List where each tuple contains the games for a user.\n",
    "    \"\"\"\n",
    "    baskets_tuples = [tuple(row) for row in baskets_df['games']]\n",
    "    return baskets_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8134c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules_df(baskets_tuples, support, apriori_confidence, rules_confidence):\n",
    "    \"\"\" \n",
    "    Generates association rules from the given baskets using the efficient apriori algorithm.\n",
    "    Parameters:\n",
    "        baskets_tuples (list of tuples): List where each tuple contains the games for a user.\n",
    "        support (float): Minimum support threshold.\n",
    "        apriori_confidence (float): Minimum confidence threshold for the rules from efficient apriori algorithm.\n",
    "        rules_confidence (float): Minimum confidence threshold for the rules from mlxtend function.\n",
    "    \"\"\"\n",
    "    itemsets, rules = apriori_efficient(baskets_tuples, min_support=support, min_confidence=apriori_confidence)\n",
    "\n",
    "    len_transactions = len(baskets_tuples)\n",
    "    itemsets_flattened = []\n",
    "    itemsets_supports = []\n",
    "    for i in itemsets.keys():\n",
    "        itemsets_flattened.extend([item[0] for item in itemsets[i].items()])\n",
    "        itemsets_supports.extend([item[1]/len_transactions for item in itemsets[i].items()])\n",
    "    \n",
    "    itemsets_dict = {\n",
    "        'support': itemsets_supports,\n",
    "        'itemsets': itemsets_flattened\n",
    "    }\n",
    "    itemsets_df = pd.DataFrame(itemsets_dict)\n",
    "\n",
    "    rules = fp.association_rules(itemsets_df, metric=\"confidence\", min_threshold=rules_confidence)\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "850832ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_association(rules_df, product_list, N=1):\n",
    "    \"\"\" \n",
    "    Recommends products based on association rules.\n",
    "    Parameters:\n",
    "        rules_df (pd.DataFrame): DataFrame containing association rules with columns 'antecedents' and 'consequents'.\n",
    "        product_list (set): Set of products already reviewed by the user.\n",
    "        N (int): Number of recommendations to return.\n",
    "    \"\"\"\n",
    "    candidate_rules = rules_df[rules_df['antecedents'].apply(lambda x: x.issubset(product_list))]\n",
    "    candidate_rules = candidate_rules.sort_values(\"confidence\")\n",
    "    recommendation_list = []\n",
    "    for i in range(len(candidate_rules)):\n",
    "        for item in candidate_rules.iloc[i]['consequents']:\n",
    "            if item not in product_list and item not in recommendation_list:\n",
    "                recommendation_list.append(item)\n",
    "            if len(recommendation_list) >= N:\n",
    "                break\n",
    "        if len(recommendation_list) >= N:\n",
    "            break\n",
    "\n",
    "    return recommendation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2c30cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, rules_df):\n",
    "    user = row['user']\n",
    "    train_items = row['games']\n",
    "    test_items = row['test']\n",
    "    recommended_items = recommender_association(rules_df, train_items, N=10)\n",
    "    return user, recommended_items, test_items\n",
    "\n",
    "def get_predictions_from_rules(rules_df, baskets_test):\n",
    "    \"\"\" \n",
    "    Generates predictions for the test baskets using the provided association rules.\n",
    "    Parameters:\n",
    "        rules_df (pd.DataFrame): DataFrame containing association rules.\n",
    "        baskets_test (pd.DataFrame): DataFrame with columns 'user', 'games', and 'test'.\n",
    "    \"\"\"\n",
    "    predictions = {\n",
    "        'user': [],\n",
    "        'recommended_items': [],\n",
    "        'true_items': []\n",
    "    }\n",
    "\n",
    "    results = Parallel(n_jobs=-1, backend='loky')(\n",
    "        delayed(lambda row: process_row(row, rules_df))(row) for _, row in baskets_test.iterrows()\n",
    "    )   \n",
    "\n",
    "    for user, recommended_items, true_items in results:\n",
    "        predictions['user'].append(user)\n",
    "        predictions['recommended_items'].append(recommended_items)\n",
    "        predictions['true_items'].append(true_items)\n",
    "        \n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0f29ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recommended_items, true_items, k):\n",
    "    \"\"\" \n",
    "    Calculates precision at k for the recommended items.\n",
    "    Parameters:\n",
    "        recommended_items (list): List of recommended items.\n",
    "        true_items (list): List of true items.\n",
    "        k (int): The cutoff rank (number of recommended items to consider).\n",
    "    \"\"\"\n",
    "    recommended_at_k = recommended_items if len(recommended_items) < k else recommended_items[:k]\n",
    "    true_positives = len(set(recommended_at_k) & set(true_items))\n",
    "    precision = true_positives / (len(recommended_at_k) if len(recommended_at_k) > 0 else 1)\n",
    "    return precision\n",
    "\n",
    "def recall_at_k(recommended_items, true_items, k):\n",
    "    \"\"\" \n",
    "    Calculates recall at k for the recommended items.\n",
    "    Parameters:\n",
    "        recommended_items (list): List of recommended items.\n",
    "        true_items (list): List of true items.\n",
    "        k (int): The cutoff rank (number of recommended items to consider).\n",
    "    \"\"\"\n",
    "    recommended_at_k = recommended_items if len(recommended_items) < k else recommended_items[:k]\n",
    "    true_positives = len(set(recommended_at_k) & set(true_items))\n",
    "    recall = true_positives / (len(true_items) if len(true_items) > 0 else 1)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2abe7a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions_df, k):\n",
    "    \"\"\" \n",
    "    Evaluates the model's predictions using precision and recall at k.\n",
    "    Parameters: \n",
    "        predictions_df (pd.DataFrame): DataFrame from get_predictions_from_rules() function.\n",
    "        k (int): The cutoff rank (number of recommended items to consider).\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for i in range(len(predictions_df)):\n",
    "        precisions.append(precision_at_k(predictions_df['recommended_items'][i], predictions_df['true_items'][i], k=k))\n",
    "        recalls.append(recall_at_k(predictions_df['recommended_items'][i], predictions_df['true_items'][i], k=k))\n",
    "\n",
    "    return np.mean(precisions), np.mean(recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8398989",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa87e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support: 0.01, Confidence: 0.7, K: 1 => Precision: 0.06798012069577565, Recall: 0.009157257945179792\n",
      "Support: 0.01, Confidence: 0.7, K: 3 => Precision: 0.06843371593105352, Recall: 0.00978592888063455\n",
      "Support: 0.01, Confidence: 0.7, K: 5 => Precision: 0.06843371593105352, Recall: 0.00978592888063455\n",
      "Support: 0.01, Confidence: 0.7, K: 7 => Precision: 0.06843371593105352, Recall: 0.00978592888063455\n",
      "Support: 0.01, Confidence: 0.7, K: 10 => Precision: 0.06843371593105352, Recall: 0.00978592888063455\n",
      "Support: 0.01, Confidence: 0.7, K: 1 => Precision: 0.06692284954182678, Recall: 0.009081290541046548\n",
      "Support: 0.01, Confidence: 0.7, K: 3 => Precision: 0.06710020691693763, Recall: 0.009667800835518254\n",
      "Support: 0.01, Confidence: 0.7, K: 5 => Precision: 0.06710020691693763, Recall: 0.009667800835518254\n",
      "Support: 0.01, Confidence: 0.7, K: 7 => Precision: 0.06710020691693763, Recall: 0.009667800835518254\n",
      "Support: 0.01, Confidence: 0.7, K: 10 => Precision: 0.06710020691693763, Recall: 0.009667800835518254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m baskets_test, baskets_train \u001b[38;5;241m=\u001b[39m transform_test_and_train_set(baskets_test, baskets_train)\n\u001b[0;32m     26\u001b[0m baskets_train_as_tuples \u001b[38;5;241m=\u001b[39m baskets_df_to_tuples(baskets_train)\n\u001b[1;32m---> 28\u001b[0m rules_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_rules_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaskets_train_as_tuples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msupport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapriori_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrules_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m rules_df\u001b[38;5;241m.\u001b[39mto_csv(generated_data_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrules_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_id\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     31\u001b[0m predictions_df \u001b[38;5;241m=\u001b[39m get_predictions_from_rules(rules_df, baskets_test)\n",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m, in \u001b[0;36mget_rules_df\u001b[1;34m(baskets_tuples, support, apriori_confidence, rules_confidence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_rules_df\u001b[39m(baskets_tuples, support, apriori_confidence, rules_confidence):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Generates association rules from the given baskets using the efficient apriori algorithm.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m        rules_confidence (float): Minimum confidence threshold for the rules from mlxtend function.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     itemsets, rules \u001b[38;5;241m=\u001b[39m \u001b[43mapriori_efficient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaskets_tuples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msupport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapriori_confidence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     len_transactions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(baskets_tuples)\n\u001b[0;32m     13\u001b[0m     itemsets_flattened \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\bloom\\anaconda3\\envs\\dtu-ml\\Lib\\site-packages\\efficient_apriori\\apriori.py:58\u001b[0m, in \u001b[0;36mapriori\u001b[1;34m(transactions, min_support, min_confidence, max_length, verbosity, output_transaction_ids)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapriori\u001b[39m(\n\u001b[0;32m     13\u001b[0m     transactions: typing\u001b[38;5;241m.\u001b[39mIterable[typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mset\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m]],\n\u001b[0;32m     14\u001b[0m     min_support: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     output_transaction_ids: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m ):\n\u001b[0;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    The classic apriori algorithm as described in 1994 by Agrawal et al.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    [{a} -> {b}]\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     itemsets, num_trans \u001b[38;5;241m=\u001b[39m \u001b[43mitemsets_from_transactions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_transaction_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     itemsets_raw \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     67\u001b[0m         length: {item: counter\u001b[38;5;241m.\u001b[39mitemset_count \u001b[38;5;28;01mfor\u001b[39;00m (item, counter) \u001b[38;5;129;01min\u001b[39;00m itemsets\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (length, itemsets) \u001b[38;5;129;01min\u001b[39;00m itemsets\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     69\u001b[0m     }\n\u001b[0;32m     70\u001b[0m     rules \u001b[38;5;241m=\u001b[39m generate_rules_apriori(itemsets_raw, min_confidence, num_trans, verbosity)\n",
      "File \u001b[1;32mc:\\Users\\bloom\\anaconda3\\envs\\dtu-ml\\Lib\\site-packages\\efficient_apriori\\itemsets.py:334\u001b[0m, in \u001b[0;36mitemsets_from_transactions\u001b[1;34m(transactions, min_support, max_length, verbosity, output_transaction_ids)\u001b[0m\n\u001b[0;32m    332\u001b[0m found_itemsets: typing\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m candidate \u001b[38;5;129;01min\u001b[39;00m C_k:\n\u001b[1;32m--> 334\u001b[0m     over_min_support, indices \u001b[38;5;241m=\u001b[39m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransaction_indices_sc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_support\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m over_min_support:\n\u001b[0;32m    336\u001b[0m         found_itemsets[candidate] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(indices)\n",
      "File \u001b[1;32mc:\\Users\\bloom\\anaconda3\\envs\\dtu-ml\\Lib\\site-packages\\efficient_apriori\\itemsets.py:79\u001b[0m, in \u001b[0;36mTransactionManager.transaction_indices_sc\u001b[1;34m(self, transaction, min_support)\u001b[0m\n\u001b[0;32m     77\u001b[0m item \u001b[38;5;241m=\u001b[39m transaction\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m     78\u001b[0m indices \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices_by_item[item])\n\u001b[1;32m---> 79\u001b[0m support \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m support \u001b[38;5;241m<\u001b[39m min_support:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bloom\\anaconda3\\envs\\dtu-ml\\Lib\\site-packages\\efficient_apriori\\itemsets.py:42\u001b[0m, in \u001b[0;36mTransactionManager.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitems\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices_by_item\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transactions\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransaction_indices\u001b[39m(\u001b[38;5;28mself\u001b[39m, transaction: typing\u001b[38;5;241m.\u001b[39mIterable[typing\u001b[38;5;241m.\u001b[39mHashable]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "baskets_df = baskets.to_pandas()\n",
    "K=20\n",
    "folds = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "support = 0.005\n",
    "confidence = 0.6\n",
    "ks = np.linspace(1, 10, 5, dtype=int)\n",
    "\n",
    "CV_results = {\n",
    "    'support': [],\n",
    "    'confidence': [],\n",
    "    'k': [],\n",
    "    'precision': [],\n",
    "    'recall': []\n",
    "}\n",
    "\n",
    "fold_id = 0\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    for train_index, test_index in folds.split(baskets_df):\n",
    "        baskets_train = baskets_df.iloc[train_index]\n",
    "        baskets_test = baskets_df.iloc[test_index]\n",
    "        baskets_test, baskets_train = transform_test_and_train_set(baskets_test, baskets_train)\n",
    "\n",
    "        baskets_train_as_tuples = baskets_df_to_tuples(baskets_train)\n",
    "\n",
    "        rules_df = get_rules_df(baskets_train_as_tuples, support, apriori_confidence=confidence, rules_confidence=confidence)\n",
    "        rules_df.to_csv(generated_data_path / f\"rules_{support}_{confidence}_fold{fold_id + 1}.csv\", index=True)\n",
    "        \n",
    "        predictions_df = get_predictions_from_rules(rules_df, baskets_test)\n",
    "        for k in ks:\n",
    "            precision, recall = evaluate_model(predictions_df, k)\n",
    "            print(f\"Support: {support}, Confidence: {confidence}, K: {k} => Precision: {precision}, Recall: {recall}\")\n",
    "            CV_results['support'].append(support)\n",
    "            CV_results['confidence'].append(confidence)\n",
    "            CV_results['k'].append(k)\n",
    "            CV_results['precision'].append(precision)\n",
    "            CV_results['recall'].append(recall)\n",
    "        fold_id += 1\n",
    "        if fold_id == 5:\n",
    "            #Check only first 5 folds\n",
    "            break\n",
    "\n",
    "CV_results_df = pd.DataFrame(CV_results)\n",
    "CV_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87170b6a",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search here is fine, but I do wonder if we could do better with a more systematic approach (would have to think about it)\n",
    "# baskets_train_as_tuples = [tuple(row) for row in baskets_train['games']]\n",
    "# supports = [0.01, 0.005]\n",
    "# confidences = [0.6, 0.7, 0.8]\n",
    "# ks = np.linspace(1, 10, 10, dtype=int)\n",
    "\n",
    "# evaluation_results = {\n",
    "#     'support': [],\n",
    "#     'confidence': [],\n",
    "#     'k': [],\n",
    "#     'precision': [],\n",
    "#     'recall': []\n",
    "# }\n",
    "\n",
    "# for support in supports:\n",
    "#     for confidence in confidences:\n",
    "#         rules_df = get_rules_df(baskets_train_as_tuples, support, confidence)\n",
    "#         pickle.dump(rules_df, open(f'data/freq_itemsets_rules_s{support}_c{confidence}.pkl', 'wb'))\n",
    "#         predictions_df = get_predictions_from_rules(rules_df, baskets_test)\n",
    "#         pickle.dump(predictions_df, open(f'data/freq_itemsets_pred_s{support}_c{confidence}.pkl', 'wb'))\n",
    "#         for k in ks:\n",
    "#             precision, recall = evaluate_model(predictions_df, k)\n",
    "#             print(f\"Support: {support}, Confidence: {confidence}, K: {k} => Precision: {precision}, Recall: {recall}\")\n",
    "#             evaluation_results['support'].append(support)\n",
    "#             evaluation_results['confidence'].append(confidence)\n",
    "#             evaluation_results['k'].append(k)\n",
    "#             evaluation_results['precision'].append(precision)\n",
    "#             evaluation_results['recall'].append(recall)\n",
    "           \n",
    "# evaluation_results_df = pd.DataFrame(evaluation_results)\n",
    "# evaluation_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
